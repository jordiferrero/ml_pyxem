{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simulated datasets\n",
    "\n",
    "For both the training and testing datasets.\n",
    "This will simplify the simulation to take a step back.\n",
    "\n",
    "The processing steps include:\n",
    "- NO Domain randomisation (a single - and the original - relrod and spot_spread values)\n",
    "- Multiple phases (6)\n",
    "- NO noise (S&P)\n",
    "- Integrating both 1D and 2D\n",
    "- NO adding background intensity for 1D case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import hyperspy.api as hs\n",
    "import pyxem as pxm\n",
    "import diffpy.structure\n",
    "from matplotlib import pyplot as plt\n",
    "from tempfile import TemporaryFile\n",
    "from diffsims.libraries.structure_library import StructureLibrary\n",
    "from diffsims.generators.diffraction_generator import DiffractionGenerator\n",
    "from diffsims.generators.library_generator import DiffractionLibraryGenerator, VectorLibraryGenerator\n",
    "from diffsims.sims.diffraction_simulation import DiffractionSimulation\n",
    "import tqdm\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Variables\n",
    "\n",
    "# Paths\n",
    "root = r'C:/Users/anish/Documents/GitHub/ml_pyxem/mini_2/'\n",
    "\n",
    "# Phases\n",
    "structures_path = os.path.join(root, 'crystal_phases')\n",
    "phase_files = ['p4mbm_tetragonal.cif',]\n",
    "add_bkg_phase = False # Do you want to add a bkg/just noise phase at the end? If True, the final datasets will be phases + 1 shape.\n",
    "\n",
    "# Calibration values\n",
    "calibrations = [0.00588]\n",
    "\n",
    "# Processing values\n",
    "n_angle_points = 10\n",
    "\n",
    "# Domain amplification\n",
    "simulated_direct_beam_bool = [False,]\n",
    "relrod_list = [0.02,]\n",
    "spot_spread_list = [0.02,]\n",
    "\n",
    "# Simulation microscope values (for azimuthal integration)\n",
    "detector_size = 515 #px\n",
    "beam_energy = 200.0 #keV\n",
    "wavelength = 2.5079e-12 #m\n",
    "detector_pix_size = 55e-6 #m\n",
    "from pyxem.detectors import Medipix515x515Detector\n",
    "detector = Medipix515x515Detector()\n",
    "\n",
    "# Noise addition values (do not change)\n",
    "add_noise = False\n",
    "include_also_non_noisy_simulation = True # If add noise, do you want to also have the non-noisy data?\n",
    "snrs = [0.9, 0.99]\n",
    "intensity_spikes = [0.25,]\n",
    "\n",
    "# Cropping and post-processing\n",
    "cropping_start_k = 0.11 #k units\n",
    "cropping_stop_k = 1.30 #k_units\n",
    "cropped_signal_k_points = 147 # To rebin signal, if necessary (when using k_units)\n",
    "\n",
    "cropping_start_px = 13.55 #pixels\n",
    "cropping_stop_px = 160.55 #pixels\n",
    "sqrt_signal = False\n",
    "\n",
    "\n",
    "# Background parameterisation values (A: pre-exp factor, tau: decay time constant)\n",
    "add_background_to = 'none' # Select from 'all', '1D_only', 'none'\n",
    "a_vals = [1., 5.]\n",
    "tau_vals = [0.5, 1.5]\n",
    "\n",
    "# Debug (save and plot files)\n",
    "save_hspy_files = False\n",
    "plot_hspy_files = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx amount of 2D diffraction patterns that will be produced: 20\n",
      "Approx memory needed: 0.021218 GB\n"
     ]
    }
   ],
   "source": [
    "val = n_angle_points * (len(phase_files) + 1)* len(relrod_list) * len(spot_spread_list) #* len(snrs) * len(intensity_spikes)\n",
    "print('Approx amount of 2D diffraction patterns that will be produced: {}'.format(val))\n",
    "memory = detector_size**2 * val * 4 / 1e9  #4 bytes per float32 value\n",
    "print('Approx memory needed: {} GB'.format(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition 1 (for the first calibration value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calibration = calibrations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data for each phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_phases = 1\n"
     ]
    }
   ],
   "source": [
    "phase_dict = {}\n",
    "for phase in phase_files:\n",
    "     name = phase.split(\".\")[0]\n",
    "     phase_dict[name] = diffpy.structure.loadStructure(os.path.join('crystal_phases', phase))\n",
    "     print('n_phases = {}'.format(len(phase_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_euler(npoints):\n",
    "    radius = 1\n",
    "    np.random.seed(1)\n",
    "    u = np.random.randint(-100,100+1,size=(npoints,))/100 \n",
    "    u2 = 2*np.pi*np.random.random(size=(npoints,))\n",
    "    theta = 2*np.pi*np.random.random(size=(npoints,))\n",
    "    x = radius*np.sqrt(1-u**2)*np.cos(theta)\n",
    "    y = radius*np.sqrt(1-u**2)*np.sin(theta)\n",
    "    z = radius*u \n",
    "    phi = np.arccos(z/radius)\n",
    "    eulerAlpha = u2\n",
    "    eulerBeta = phi\n",
    "    eulerGamma = theta\n",
    "    return np.array([np.rad2deg(eulerAlpha),np.rad2deg(eulerBeta),np.rad2deg(eulerGamma)]).T \n",
    "\n",
    "\n",
    "def get_reciprocal_radius(detector_size, calibration):\n",
    "    half_pattern_size = detector_size // 2\n",
    "    reciprocal_radius = calibration * half_pattern_size\n",
    "    return reciprocal_radius\n",
    "\n",
    "\n",
    "def create_diffraction_library(phase_dict, euler_list,\n",
    "                                       beam_energy, relrod_length,\n",
    "                                       calibration, detector_size,\n",
    "                                       with_direct_beam):\n",
    "\n",
    "    phase_names = list(phase_dict.keys())\n",
    "    phases = list(phase_dict.values())\n",
    "    euler_list_n = [euler_list, ] * len(phase_names)\n",
    "\n",
    "    sample_lib = StructureLibrary(phase_names, phases, euler_list_n)\n",
    "    ediff = DiffractionGenerator(beam_energy)#, relrod_length)\n",
    "    diff_gen = DiffractionLibraryGenerator(ediff)\n",
    "\n",
    "    reciprocal_radius = get_reciprocal_radius(detector_size, calibration)\n",
    "    library = diff_gen.get_diffraction_library(sample_lib,\n",
    "                                               calibration=calibration,\n",
    "                                               reciprocal_radius=reciprocal_radius,\n",
    "                                               half_shape=(detector_size//2, detector_size//2),\n",
    "                                               with_direct_beam=with_direct_beam)\n",
    "    return library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "data = {}\n",
    "for key, val in phase_dict.items():\n",
    "    data[key] = []\n",
    "for with_direct_beam in simulated_direct_beam_bool:\n",
    "    for relrod_length in tqdm.tqdm(relrod_list):\n",
    "        for spot_spread in spot_spread_list:\n",
    "\n",
    "            euler_list = get_random_euler(n_angle_points)\n",
    "\n",
    "            library = create_diffraction_library(phase_dict, euler_list,\n",
    "                                                 beam_energy, relrod_length,\n",
    "                                                 calibration, detector_size,\n",
    "                                                 with_direct_beam)\n",
    "\n",
    "            reciprocal_radius = get_reciprocal_radius(detector_size, calibration)\n",
    "            print(library)\n",
    "            for euler in euler_list:\n",
    "                for phase in library.keys():\n",
    "                    pattern = DiffractionSimulation.get_diffraction_pattern(library.get_library_entry(phase=phase,angle=euler)['Sim'])\n",
    "                    data[phase].append(pattern)\n",
    "                    #DiffractionSimulation.plot(pattern)\n",
    "                    plt.figure()\n",
    "                    plt.imshow(pattern, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p4mbm_tetragonal': [array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Chunks do not add up to shape. Got chunks=((10, 515, 515),), shape=(0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z3/ym7_vbgd7mbdj7g3t95hznjr0000gn/T/ipykernel_64554/4087653978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlist_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dask/array/core.py\u001b[0m in \u001b[0;36mfrom_array\u001b[0;34m(x, chunks, name, lock, asarray, fancy, getitem, meta, inline_array)\u001b[0m\n\u001b[1;32m   3197\u001b[0m     \u001b[0mprevious_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chunks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3199\u001b[0;31m     chunks = normalize_chunks(\n\u001b[0m\u001b[1;32m   3200\u001b[0m         \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprevious_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3201\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dask/array/core.py\u001b[0m in \u001b[0;36mnormalize_chunks\u001b[0;34m(chunks, shape, limit, dtype, previous_chunks)\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m         ):\n\u001b[0;32m-> 2852\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2853\u001b[0m                 \u001b[0;34m\"Chunks do not add up to shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m                 \u001b[0;34m\"Got chunks=%s, shape=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Chunks do not add up to shape. Got chunks=((10, 515, 515),), shape=(0,)"
     ]
    }
   ],
   "source": [
    "# Stack data\n",
    "import dask.array as da\n",
    "\n",
    "for i, value in enumerate(data.values()):\n",
    "    list_data = da.from_array([x.data for x in value], chunks=(10, detector_size, detector_size))\n",
    "\n",
    "    if i ==0:\n",
    "        #list_data = np.expand_dims(list_data, 1)\n",
    "        training_data = list_data\n",
    "    else:\n",
    "        #list_data = np.expand_dims(list_data, 1)\n",
    "        training_data = da.vstack([training_data, list_data],)\n",
    "\n",
    "del data\n",
    "del library\n",
    "del list_data\n",
    "gc.collect()\n",
    "\n",
    "shape = (len(phase_dict.keys()),\n",
    "         n_angle_points*len(relrod_list)*len(spot_spread_list)*len(simulated_direct_beam_bool),\n",
    "         detector_size,\n",
    "         detector_size)\n",
    "\n",
    "training_data = training_data.reshape(shape)\n",
    "training_data = pxm.LazyElectronDiffraction2D(training_data)\n",
    "training_data.set_diffraction_calibration(calibration)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shiftList = np.zeros((np.size(training_data.data,0),\n",
    "                      np.size(training_data.data,1),\n",
    "                      2,)\n",
    "                     )\n",
    "\n",
    "shiftList[:,:,0]=0.5\n",
    "shiftList[:,:,1]=0.5\n",
    "\n",
    "shiftList = shiftList.reshape(-1, shiftList.shape[-1]) # Flatten the 2D navigtion axis\n",
    "\n",
    "training_data.compute()\n",
    "training_data.align2D(shifts=shiftList,crop=False,fill_value=0., parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save_hspy_files:\n",
    "    name = '2D_hspy_simdata_{}classes_{}neuler_{}cal_{}relrod_{}spotsize.hspy'.format(\n",
    "        len(phase_dict), n_angle_points, calibration, relrod_list, spot_spread_list)\n",
    "\n",
    "    training_data.save(name, overwrite=True)\n",
    "if plot_hspy_files:\n",
    "    training_data.plot(cmap='viridis')\n",
    "\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add background phase (without signal)\n",
    "\n",
    "Create a blank detector in which noise and a bkg will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Only if `add_bkg_phase` is True. Otherwise no changes.\n",
    "if add_bkg_phase:\n",
    "    # Add phase in the dictionary\n",
    "    phase_dict['bkg_phase'] = []\n",
    "\n",
    "    # Create blank detector\n",
    "    shape_blank = np.shape(training_data,)[1:]\n",
    "    shape_blank = (1,) + shape_blank\n",
    "    blank = pxm.signals.electron_diffraction2d.ElectronDiffraction2D(np.zeros(shape_blank))\n",
    "    training_data = hs.stack([training_data, blank], axis=1)\n",
    "\n",
    "print(len(phase_dict))\n",
    "training_data.data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise\n",
    "\n",
    "In two steps:\n",
    "- S&P noise\n",
    "- Poisson noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_noise_to_simulation(simulation_arr, snr, int_salt,):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Salt and pepper\n",
    "    def addsalt_pepper(dp_arr, snr, int_min = 0, int_max = int_salt,):\n",
    "\n",
    "        p0 = snr\n",
    "        # Add noise\n",
    "        size = np.shape(dp_arr)\n",
    "        mask = np.random.choice(a=(0, 1, 2),\n",
    "                                size=size,\n",
    "                                p=[p0, (1 - p0) / 2., (1 - p0) / 2.])\n",
    "\n",
    "        im = dp_arr.copy()\n",
    "        #im[mask == 1] = int_min # salt noise\n",
    "        im[mask == 2] = int_max # pepper noise\n",
    "\n",
    "        return im\n",
    "\n",
    "    # Add poisson noise on sp noise and normalise\n",
    "    im = simulation_arr.copy()\n",
    "    im += np.random.poisson(im)\n",
    "\n",
    "    max = im.max()\n",
    "    if max == 0:\n",
    "        im = im\n",
    "    else:\n",
    "        im = im / im.max()\n",
    "\n",
    "    # Add bright spots randomly accross detector\n",
    "    im_sp = addsalt_pepper(im, snr,)\n",
    "\n",
    "    return im_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z3/ym7_vbgd7mbdj7g3t95hznjr0000gn/T/ipykernel_31670/2676339999.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Map the noise addition function on signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtraining_data_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Include the non-corrupted data in the dataset?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'add_noise' is not defined"
     ]
    }
   ],
   "source": [
    "# Map the noise addition function on signal\n",
    "if add_noise:\n",
    "    training_data_noisy = []\n",
    "\n",
    "    # Include the non-corrupted data in the dataset?\n",
    "    if include_also_non_noisy_simulation:\n",
    "        training_data_noisy.append(training_data)\n",
    "\n",
    "    # Append noisy data\n",
    "    for snr in snrs:\n",
    "        for int_spike in intensity_spikes:\n",
    "\n",
    "            signal_noisy = training_data.map(add_noise_to_simulation,\n",
    "                                             snr=snr, int_salt=int_spike,\n",
    "                                             inplace=False, parallel=True)\n",
    "\n",
    "            training_data_noisy.append(signal_noisy)\n",
    "\n",
    "    del training_data\n",
    "    del signal_noisy\n",
    "    gc.collect()\n",
    "\n",
    "    training_data_noisy = hs.stack(training_data_noisy, axis=0)\n",
    "\n",
    "else:\n",
    "    # No noise addition\n",
    "    training_data_noisy = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save_hspy_files and add_noise:\n",
    "    name = '2D_hspy_simdata_{}classes_{}neuler_{}cal_{}relrod_{}spotsize_withNoise.hspy'.format(\n",
    "        len(phase_dict), n_angle_points, calibration, relrod_list, spot_spread_list)\n",
    "\n",
    "    training_data_noisy.save(name, overwrite=True)\n",
    "if plot_hspy_files and add_noise:\n",
    "    training_data_noisy.plot(cmap='viridis')\n",
    "\n",
    "print(training_data_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate radially 2D (cake data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "camera_length = detector_pix_size / (wavelength * calibration * 1e10)\n",
    "training_data_noisy.unit = \"k_A^-1\"\n",
    "training_data_noisy.set_experimental_parameters(beam_energy=beam_energy)\n",
    "radial_steps = int(np.ceil((int(detector_size/2) - 1)/2)*2)\n",
    "training_data_2D = training_data_noisy.get_azimuthal_integral2d(npt_rad=radial_steps,\n",
    "                                                          center=([detector_size/2,detector_size/2]),\n",
    "                                                          detector=detector,\n",
    "                                                          detector_dist=camera_length,\n",
    "                                                          map_kwargs={'parallel':True})\n",
    "print(training_data_2D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate radially 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "camera_length = detector_pix_size / (wavelength * calibration * 1e10)\n",
    "training_data_noisy.unit = \"k_A^-1\"\n",
    "training_data_noisy.set_experimental_parameters(beam_energy=beam_energy)\n",
    "radial_steps = int(np.ceil((int(detector_size/2) - 1)/2)*2)\n",
    "training_data_1D = training_data_noisy.get_azimuthal_integral1d(npt_rad=radial_steps,\n",
    "                                                          center=([detector_size/2,detector_size/2]),\n",
    "                                                          detector=detector,\n",
    "                                                          detector_dist=camera_length,\n",
    "                                                          map_kwargs={'parallel':True})\n",
    "print(training_data_1D)\n",
    "\n",
    "del training_data_noisy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save_hspy_files:\n",
    "    name = '1D_hspy_simdata_{}classes_{}neuler_{}cal_{}relrod_{}spotsize.hspy'.format(\n",
    "        len(phase_dict), n_angle_points, calibration, relrod_list, spot_spread_list)\n",
    "\n",
    "    training_data_1D.save(name, overwrite=True)\n",
    "if plot_hspy_files:\n",
    "    training_data_1D.plot()\n",
    "\n",
    "print(training_data_1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise (and sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2D cake dataset\n",
    "# Sqrt signal (if wanted)\n",
    "if sqrt_signal:\n",
    "    training_data_2D.data = np.sqrt(training_data_2D.data)\n",
    "\n",
    "# Normalise\n",
    "def norm_2d(arr):\n",
    "    return arr / arr.max()\n",
    "\n",
    "training_data_2D_norm = training_data_2D.map(norm_2d, inplace=False)\n",
    "training_data_2D_norm = training_data_2D_norm.data\n",
    "\n",
    "# Correct any nan value\n",
    "nan_mask = np.isnan(training_data_2D_norm)\n",
    "training_data_2D_norm[nan_mask] = 0\n",
    "\n",
    "print(training_data_2D_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1D dataset\n",
    "# Sqrt signal (if wanted)\n",
    "if sqrt_signal:\n",
    "    training_data_1D.data = np.sqrt(training_data_1D.data)\n",
    "\n",
    "# Normalise\n",
    "dpmax = training_data_1D.data.max(2)\n",
    "training_data_1D_norm = training_data_1D.data/dpmax[:,:,np.newaxis]\n",
    "\n",
    "# Correct any nan value\n",
    "nan_mask = np.isnan(training_data_1D_norm)\n",
    "training_data_1D_norm[nan_mask] = 0\n",
    "\n",
    "print(training_data_1D_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add simulated background\n",
    "\n",
    "Approximate background as a $A*exp^{(-tau \\: q)}$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_background_to_signal_array(normalised_sim_data_array, x_axis,\n",
    "                                     a_val, tau_val, bkg_function='exp_decay', dimensions=1):\n",
    "    \"\"\"\n",
    "    :param normalised_sim_data_array:\n",
    "        The normalised 1d signal array (nav axis should be (points, phases, q))\n",
    "    :param x_axis: array of the actual q values\n",
    "        The A and tau values are optimised for 1/A-1 magnitude\n",
    "    :return: extended signal with new sets of sim data without and with bakgrounds\n",
    "    \"\"\"\n",
    "    def inv_q(x, A, tau):\n",
    "        return A * x**(-tau)\n",
    "\n",
    "    def exp_decay(x, A, tau):\n",
    "        return A * np.exp(- tau * x)\n",
    "\n",
    "    if bkg_function == 'exp_decay':\n",
    "        bkg = exp_decay(x_axis, a_val, tau_val)\n",
    "    elif bkg_function == 'inv_q':\n",
    "        bkg = inv_q(x_axis, a_val, tau_val)\n",
    "\n",
    "    if dimensions == 1:\n",
    "        return normalised_sim_data_array + bkg\n",
    "\n",
    "    elif dimensions == 2:\n",
    "        n = normalised_sim_data_array.shape[-1]\n",
    "        bkg = np.tile(bkg, (n,1)).T\n",
    "        return normalised_sim_data_array + bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For 2D cake dataset\n",
    "# Expand datasets by copying and adding bkg\n",
    "training_data_2D_norm_bkg = training_data_2D_norm\n",
    "\n",
    "if add_background_to == 'all':\n",
    "    # Get the x-axis values from which to calculate bkg\n",
    "    qs = training_data_2D.axes_manager.signal_axes[1].axis\n",
    "    qs\n",
    "\n",
    "    # Add bkg to signal\n",
    "    for a in a_vals:\n",
    "        for tau in tau_vals:\n",
    "            bkg_data = add_background_to_signal_array(training_data_2D_norm, qs, a, tau, dimensions=2)\n",
    "            training_data_2D_norm_bkg = np.hstack((training_data_2D_norm_bkg, bkg_data))\n",
    "\n",
    "\n",
    "training_data_2D_norm_bkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For 1D dataset\n",
    "# Expand datasets by copying and adding bkg\n",
    "training_data_1D_norm_bkg = training_data_1D_norm\n",
    "\n",
    "if add_background_to != 'none':\n",
    "    # Get the x-axis values from which to calculate bkg\n",
    "    qs = training_data_1D.axes_manager.signal_axes[0].axis\n",
    "    qs\n",
    "\n",
    "    # Add bkg to signal\n",
    "    for a in a_vals:\n",
    "        for tau in tau_vals:\n",
    "            bkg_data = add_background_to_signal_array(training_data_1D_norm, qs, a, tau)\n",
    "            training_data_1D_norm_bkg = np.hstack((training_data_1D_norm_bkg, bkg_data))\n",
    "\n",
    "    del bkg_data\n",
    "    gc.collect()\n",
    "\n",
    "training_data_1D_norm_bkg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop, rebin and renormalise\n",
    "\n",
    "Crop both in terms of q (rebin but no shift) and pixel values (shift but no rebin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2D cake dataset\n",
    "training_data_2D_norm_bkg = hs.signals.Signal2D(training_data_2D_norm_bkg)\n",
    "\n",
    "# Crop in pixel units:\n",
    "training_data_2D_px = training_data_2D_norm_bkg.isig[:, cropping_start_px: cropping_stop_px]\n",
    "\n",
    "# Renormalise data\n",
    "def norm_2d(arr):\n",
    "    return arr / arr.max()\n",
    "\n",
    "training_data_2D_px.map(norm_2d, inplace=True)\n",
    "training_data_2D_px = training_data_2D_px.data\n",
    "\n",
    "# In k units:\n",
    "# Recreate .hspy object to crop with k units\n",
    "scale = training_data_2D.axes_manager.signal_axes[1].scale\n",
    "offset = training_data_2D.axes_manager.signal_axes[1].offset\n",
    "training_data_2D_norm_bkg.axes_manager.signal_axes[1].scale = scale\n",
    "training_data_2D_norm_bkg.axes_manager.signal_axes[1].offset = offset\n",
    "\n",
    "# Crop in k units\n",
    "training_data_2D_norm_bkg.crop(axis = 3, start = cropping_start_k, end = cropping_stop_k)\n",
    "# Rebin the k units\n",
    "scale_rebin = training_data_2D_norm_bkg.data.shape[-2] / cropped_signal_k_points\n",
    "\n",
    "training_data_2D_q = training_data_2D_norm_bkg.rebin(scale=(1,1,1,scale_rebin))\n",
    "\n",
    "# Renormalise data\n",
    "training_data_2D_q.map(norm_2d, inplace=True)\n",
    "training_data_2D_q = training_data_2D_q.data\n",
    "\n",
    "del training_data_2D\n",
    "del training_data_2D_norm\n",
    "del training_data_2D_norm_bkg\n",
    "gc.collect()\n",
    "\n",
    "print(training_data_2D_q.shape)\n",
    "print(training_data_2D_px.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1D dataset\n",
    "training_data_1D_norm_bkg = hs.signals.Signal1D(training_data_1D_norm_bkg)\n",
    "\n",
    "training_data_1D_px = training_data_1D_norm_bkg.deepcopy()\n",
    "\n",
    "# Recreate .hspy object to crop with k units\n",
    "scale = training_data_1D.axes_manager.signal_axes[0].scale\n",
    "offset = training_data_1D.axes_manager.signal_axes[0].offset\n",
    "training_data_1D_norm_bkg.axes_manager.signal_axes[0].scale = scale\n",
    "training_data_1D_norm_bkg.axes_manager.signal_axes[0].offset = offset\n",
    "\n",
    "training_data_1D_q = training_data_1D_norm_bkg.deepcopy()\n",
    "\n",
    "del training_data_1D\n",
    "del training_data_1D_norm\n",
    "del training_data_1D_norm_bkg\n",
    "gc.collect()\n",
    "\n",
    "# In k units:\n",
    "# Crop in k units\n",
    "training_data_1D_q.crop_signal1D(cropping_start_k, cropping_stop_k)\n",
    "# Rebin\n",
    "scale_rebin = training_data_1D_q.data.shape[-1] / cropped_signal_k_points\n",
    "scale_rebin\n",
    "training_data_1D_q = training_data_1D_q.rebin(scale=(1,1,scale_rebin))\n",
    "# Renormalise data\n",
    "dpmax = training_data_1D_q.data.max(-1)\n",
    "training_data_1D_q = training_data_1D_q.data/dpmax[:,:,np.newaxis]\n",
    "\n",
    "# In pixel units:\n",
    "# Crop in pixel units\n",
    "training_data_1D_px.crop_signal1D(cropping_start_px, cropping_stop_px)\n",
    "# Renormalise data\n",
    "dpmax = training_data_1D_px.data.max(-1)\n",
    "training_data_1D_px = training_data_1D_px.data/dpmax[:,:,np.newaxis]\n",
    "\n",
    "print(training_data_1D_q.shape)\n",
    "print(training_data_1D_px.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN requirements: reshape and labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phase_names = list(phase_dict.keys())\n",
    "\n",
    "print(phase_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2D cake dataset\n",
    "\n",
    "shape_q = (np.prod(training_data_2D_q.shape[:-2]),) + training_data_2D_q.shape[-2:]\n",
    "shape_px = (np.prod(training_data_2D_px.shape[:-2]),) + training_data_2D_px.shape[-2:]\n",
    "\n",
    "training_data_2D_q = training_data_2D_q.reshape(shape_q)\n",
    "training_data_2D_px = training_data_2D_px.reshape(shape_px)\n",
    "\n",
    "print(training_data_2D_q.shape)\n",
    "print(training_data_2D_px.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create labels for 2D\n",
    "n_phases = len(phase_dict)\n",
    "labels_2D = np.zeros((n_phases, int(training_data_2D_q.shape[0]/n_phases)))\n",
    "for i in range(n_phases):\n",
    "    labels_2D[i,:] = i\n",
    "\n",
    "training_labels_2D = labels_2D.flatten()\n",
    "training_labels_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1D dataset\n",
    "training_data_1D_q = training_data_1D_q.reshape(-1, training_data_1D_q.shape[-1])\n",
    "training_data_1D_px = training_data_1D_px.reshape(-1, training_data_1D_px.shape[-1])\n",
    "\n",
    "print(training_data_1D_q.shape)\n",
    "print(training_data_1D_px.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create labels for 1D\n",
    "n_phases = len(phase_dict)\n",
    "labels = np.zeros((n_phases, int(training_data_1D_q.shape[0]/n_phases)))\n",
    "for i in range(n_phases):\n",
    "    labels[i,:] = i\n",
    "\n",
    "training_labels = labels.flatten()\n",
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save 1D datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for outliers and nan values\n",
    "where_nan_q = np.argwhere(np.isnan(training_data_1D_q))\n",
    "where_nan_px = np.argwhere(np.isnan(training_data_1D_px))\n",
    "\n",
    "training_data_1D_q = np.delete(training_data_1D_q, where_nan_q[:,0], axis = 0)\n",
    "training_labels_q = np.delete(training_labels, where_nan_q[:,0], axis = 0)\n",
    "\n",
    "training_data_1D_px = np.delete(training_data_1D_px, where_nan_px[:,0], axis = 0)\n",
    "training_labels_px = np.delete(training_labels, where_nan_px[:,0], axis = 0)\n",
    "\n",
    "print(training_data_1D_q.shape, training_labels_q.shape)\n",
    "print(training_data_1D_px.shape, training_labels_px.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "store_train_data = TemporaryFile()\n",
    "x = training_data_1D_q\n",
    "y = training_labels_q\n",
    "\n",
    "np.savez('1D_simulated_data_cal{}_cropK_{}classes_{}neuler'.format(calibration,\n",
    "                                                                              n_phases,\n",
    "                                                                        n_angle_points,),\n",
    "         x=x, y=y, phases=phase_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "store_train_data = TemporaryFile()\n",
    "x = training_data_1D_px\n",
    "y = training_labels_px\n",
    "\n",
    "np.savez('1D_simulated_data_cal{}_cropPX_{}classes_{}neuler'.format(calibration,\n",
    "                                                                              n_phases,\n",
    "                                                                        n_angle_points,),\n",
    "         x=x, y=y, phases=phase_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save 2D datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for outliers and nan values\n",
    "where_nan_q = np.argwhere(np.isnan(training_data_2D_q))\n",
    "where_nan_px = np.argwhere(np.isnan(training_data_2D_px))\n",
    "\n",
    "training_data_2D_q = np.delete(training_data_2D_q, where_nan_q[:,0], axis = 0)\n",
    "training_labels_q = np.delete(training_labels_2D, where_nan_q[:,0], axis = 0)\n",
    "\n",
    "training_data_2D_px = np.delete(training_data_2D_px, where_nan_px[:,0], axis = 0)\n",
    "training_labels_px = np.delete(training_labels_2D, where_nan_px[:,0], axis = 0)\n",
    "\n",
    "print(training_data_2D_q.shape, training_labels_q.shape)\n",
    "print(training_data_2D_px.shape, training_labels_px.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "store_train_data = TemporaryFile()\n",
    "#x = training_data_2D_q\n",
    "#y = training_labels_q\n",
    "\n",
    "np.savez('2D_simulated_data_cal{}_cropK_{}classes_{}neuler'.format(calibration,\n",
    "                                                                              n_phases,\n",
    "                                                                        n_angle_points,),\n",
    "         x=training_data_2D_q,\n",
    "         y=training_labels_q,\n",
    "         phases=phase_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "store_train_data = TemporaryFile()\n",
    "#x = training_data_2D_px\n",
    "#y = training_labels_px\n",
    "\n",
    "np.savez('2D_simulated_data_cal{}_cropPX_{}classes_{}neuler'.format(calibration,\n",
    "                                                                              n_phases,\n",
    "                                                                        n_angle_points,),\n",
    "         x=training_data_2D_px,\n",
    "         y=training_labels_px,\n",
    "         phases=phase_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.figure()\n",
    "plt.plot(training_data_1D_px[i], label='px')\n",
    "plt.plot(training_data_1D_q[i], label='q')\n",
    "plt.legend()\n",
    "plt.savefig('1D_plt_compare_k_q_cropping_i{}.png'.format(i))\n",
    "\n",
    "\n",
    "f,axs = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
    "axs[0].imshow(training_data_2D_px[i],)\n",
    "axs[0].set_title('px')\n",
    "axs[1].imshow(training_data_2D_q[i],)\n",
    "axs[1].set_title('q')\n",
    "plt.xlabel('Angle')\n",
    "plt.ylabel('Radius')\n",
    "for ax in axs:\n",
    "    ax.axhline(43, ls='--', c='w', lw=0.25)\n",
    "\n",
    "plt.savefig('2D_plt_compare_k_q_cropping_i{}.png'.format(i))\n",
    "\n",
    "del training_data_1D_px\n",
    "del training_data_1D_q\n",
    "del training_data_2D_px\n",
    "del training_data_2D_q\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
